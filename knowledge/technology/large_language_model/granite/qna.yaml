created_by: abrahammie
version: 3
domain: large-language-model
document_outline: Knowledge contribution about the IBM Granite model
seed_examples:
  - context: >-
      IBM Granite is a series of decoder-only AI foundation models created by
      IBM. It was announced on September 7, 2023, and an initial paper was
      published 4 days later.
    questions_and_answers:
      - question: What is IBM Granite?
        answer: >-
          IBM Granite is a series of decoder-only AI foundation models created
          by IBM.
      - question: When was granite announced?
        answer: September 7, 2023
      - question: What's IBM's series of decoder-only AI foundation models called?
        answer: IBM Granite
  - context: >-
      Initially intended for use in IBM's cloud-based data and generative AI
      platform Watsonx along with other models, IBM opened the source code of
      some code models. Granite models are trained on datasets curated from
      Internet, academic publishings, code datasets, legal and finance
      documents.
    questions_and_answers:
      - question: What was IBM Granite initially intended for use in?
        answer: IBM's cloud-based data and generative AI platform Watsonx.
      - question: What datasets are Granite models trained on?
        answer: >-
          Curated datasets from Internet, academic publishings, code datasets,
          legal and finance documents.
      - question: Are IBM Granite models open source?
        answer: Yes IBM opened the source code of some code models.
  - context: >-
      A foundation model is an AI model trained on broad data at scale such that
      it can be adapted to a wide range of downstream tasks. Granite's first
      foundation models were Granite.13b.instruct and Granite.13b.chat. The
      "13b" in their name comes from 13 billion, the amount of parameters they
      have as models, lesser than most of the larger models of the time. Later
      models vary from 3 to 34 billion parameters.
    questions_and_answers:
      - question: What is a foundation model?
        answer: >-
          An AI model trained on broad data so it can be adapted to a wide range
          of tasks.
      - question: 'What were IBM''s first foundation models? '
        answer: Granite.13b.instruct and Granite.13b.chat.
      - question: What does the 13b in the foundation model's name mean?
        answer: It means 13 billion, the amount of parameters they have as models.
  - context: >-
      On May 6, 2024, IBM released the source code of four variations of Granite
      Code Models under Apache 2, an open source permissive license that allows
      completely free use, modification and sharing of the software, and put
      them on Hugging Face for public use.
    questions_and_answers:
      - question: When did IBM release the source code for Granite code models?
        answer: May 6, 2024
      - question: >-
          How many variations of Granite code models' source code were released
          in May 2024?
        answer: 'Four variations '
      - question: Where were Granite Code Models released for public use?
        answer: Hugging face
  - context: >-
      According to IBM's own report, Granite 8b outperforms Llama 3 on several
      coding related tasks within similar range of parameters.
    questions_and_answers:
      - question: Who reported that Granite 8b outperforms Llama 3?
        answer: IBM
      - question: What tasks did IBM report that Granite 8b outperforms Llama 3 on?
        answer: Several Coding related tasks
      - question: >-
          What model did IBM report that it's Granite 8b outperforms on several
          coding related tasks within a similar range of parameters?
        answer: Llama 3
document:
  repo: https://github.com/abrahammie/taxonomy-knowledge-docs
  commit: 658dae2c53938b2240d79f3118e56245678b457a
  patterns:
    - Granite-20250105T183103526.md
